{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skimage.data import astronaut\n",
    "from skimage.segmentation import slic\n",
    "from scipy.spatial import Delaunay\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import label2rgb\n",
    "from skimage.transform import resize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#define globals required through out the whole program\n",
    "edges           = [] #containing all edge tuple\n",
    "attrs           = [] #countaining list of attribute of all nodes\n",
    "graph_id        = 1 #id of latest graph\n",
    "node_id         = 0 #id of latest node\n",
    "graph_indicator = [] #containing graph-id for each node\n",
    "node_labels     = [] #containing labels for all node\n",
    "graph_labels    = []#containing labels for all graph\n",
    "edge_labels = []\n",
    "edge_attrs = []\n",
    "node_edge_attrs = []\n",
    "\n",
    "# Selecting dataset and k\n",
    "dataset_name = 'BUSI-with-GT'  # 'BUSI-with-GT' or  'BUSBRA' \n",
    "k = 6  # '2' '4' '6' '8'\n",
    "\n",
    "mask_dir = 'Dataset/'+ dataset_name + '/original/Masks/'\n",
    "patch_dir  = 'Dataset/'+ dataset_name + '/data-edge/k' + str(k) + '/train/patch-image/'\n",
    "\n",
    "#working directories\n",
    "\n",
    "benign_dir  = 'Dataset/'+ dataset_name + '/original/train/benign'\n",
    "malignant_dir  = 'Dataset/'+ dataset_name + '/original/train/malignant'\n",
    "normal_dir  = 'Dataset/'+ dataset_name + '/original/train/normal'\n",
    "\n",
    "#Output directories\n",
    "sourcepath='Dataset/'+ dataset_name + '/data-edge/k' + str(k) + '/train/raw'\n",
    "if not os.path.exists(sourcepath):\n",
    "    os.makedirs(sourcepath, exist_ok=False)\n",
    "\n",
    "\n",
    "node_file_name = []\n",
    "image_file_name = []\n",
    "\n",
    "#activity-label vs activity-name mapping (4-class)\n",
    "activity_map    = {}\n",
    "activity_map[1] = 'benign'\n",
    "activity_map[2] = 'malignant'\n",
    "activity_map[3] = 'normal'\n",
    "\n",
    "\n",
    "#\n",
    "def normalize_rows(arr):\n",
    "    min_values = arr.min(axis=1)\n",
    "    max_values = arr.max(axis=1)\n",
    "    normalized_arr = (arr - min_values[:, np.newaxis]) / (max_values - min_values)[:, np.newaxis]\n",
    "    return normalized_arr\n",
    "\n",
    "def find_k_min_indices_desc(array, k):\n",
    "    if k > len(array):\n",
    "        raise ValueError(\"K is larger than the size of the array.\")\n",
    "    sorted_indices = np.argsort(array)\n",
    "    k_min_indices = sorted_indices[:k]\n",
    "    return k_min_indices\n",
    "\n",
    "\n",
    "def resize_superpixel(superpixel, target_size):\n",
    "    # Calculate the resize ratio\n",
    "    \n",
    "\n",
    "    # Resize the superpixel using bilinear interpolation\n",
    "    resized_superpixel = resize(superpixel, (target_size, target_size), \n",
    "                                mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    return resized_superpixel\n",
    "\n",
    "def extract_features(model, img_array):\n",
    "    # img = image.load_img(image_path, target_size=(image_shape[0], image_shape[1]))\n",
    "    # img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    features = model.predict(img_array, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "#z-score normalization\n",
    "def normalize(arr):\n",
    "    arr = np.array(arr)\n",
    "    m   = np.mean(arr)\n",
    "    s   = np.std(arr)\n",
    "    return np.round((arr - m)/s, 3)\n",
    "\n",
    "def is_coordinate_in_white_region(image, x, y):\n",
    "\n",
    "    # Check each coordinate\n",
    "    \n",
    "    # Check if the coordinate is within the image bounds\n",
    "        # Check if the pixel is white (assuming white is represented as 255)\n",
    "\n",
    "    if image[int(x), int(y)] == 255:\n",
    "        \n",
    "        mask_check = True\n",
    "    else: \n",
    "\n",
    "        mask_check = False\n",
    "\n",
    "    return mask_check\n",
    "\n",
    "def replace_prefix(filename, new_prefix):\n",
    "    # Split the filename into prefix and the rest\n",
    "    parts = filename.split('_', 1)\n",
    "    \n",
    "    # If the filename doesn't have a prefix, return it as is\n",
    "    if len(parts) < 2:\n",
    "        return filename\n",
    "    \n",
    "    # Return the new filename with the replaced prefix\n",
    "    return new_prefix + '_' + parts[1]\n",
    "\n",
    "def get_filename(directory):\n",
    "    # Split the directory string by '/'\n",
    "    parts = directory.split('/')\n",
    "    \n",
    "    # The filename is the last part of the directory\n",
    "    filename = parts[-1]\n",
    "    \n",
    "    # Return the filename\n",
    "    return filename\n",
    "\n",
    "#generate graph for a given edge-image file\n",
    "def generate_graphs(filename, node_label, activity_map):\n",
    "    print(\" ... Reading image: \" + filename+\" ...\")\n",
    "    global node_id, edges, attrs, graph_id, node_labels, graph_indicator, edge_labels, edge_attrs, node_edge_attrs, mask_dir, node_file_name, image_file_name, k\n",
    "    cnt           = 0\n",
    "    img           = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    dim1, dim2, _ = img.shape\n",
    "\n",
    "    saved_filename = get_filename(filename)\n",
    "\n",
    "    image_mask_name = get_filename(filename)\n",
    "    image_mask_name = replace_prefix(image_mask_name, 'mask')\n",
    "    image_mask_name = mask_dir + image_mask_name\n",
    "    image_mask = cv2.imread(image_mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    attrs1        = []\n",
    "    edge = 0\n",
    "    # Assuming you have an image loaded as 'image'\n",
    "    image = img\n",
    "    # Step 1: Apply SLIC to get initial superpixels\n",
    "    segments_slic = slic(image, n_segments=150, compactness=50)\n",
    "    # Convert segments back to an image\n",
    "    segmented_image = label2rgb(segments_slic, image, kind='avg')\n",
    "\n",
    "    segments_ids = np.unique(segments_slic)\n",
    "\n",
    "    # centers\n",
    "    centers = np.array([np.mean(np.nonzero(segments_slic==i),axis=1) for i in segments_ids])\n",
    "\n",
    "    # Calculate the average intensity for each superpixel\n",
    "    unique_segments = np.unique(segments_slic)\n",
    "    average_intensities = []\n",
    "\n",
    "    print(\"Image type: \" + activity_map[node_label] + \"\\nPixel matrix is of: \" + str(dim1) + \"x\" + str(dim2))\n",
    "\n",
    "    for seg_id in unique_segments:\n",
    "        mask = segments_slic == seg_id\n",
    "        average_intensity = np.mean(image[mask], axis=0)\n",
    "        average_intensity = average_intensity.astype(int)\n",
    "        average_intensities.append(average_intensity)\n",
    "\n",
    "    average_intensities = np.array(average_intensities)\n",
    "\n",
    "    # Compute distances between each pair of superpixels based on Average intensity superpixel\n",
    "    distances_intensity = normalize_rows(cdist(average_intensities, average_intensities, metric='euclidean'))\n",
    "\n",
    "    # Compute distances between each pair of superpixels based on location of Center\n",
    "    distances_pos = normalize_rows(cdist(centers, centers, metric='euclidean'))\n",
    "\n",
    "    # Combine distance\n",
    "    distances = (distances_pos + distances_intensity)/2\n",
    "\n",
    "\n",
    "    for i in unique_segments:       #Scan all the superpixel in image\n",
    "\n",
    "        edges_image = find_k_min_indices_desc(distances[i-1], k+1)\n",
    "        edges_image = edges_image + 1\n",
    "        edges_image = edges_image + node_id\n",
    "\n",
    "        for j in edges_image:        # Forming edges\n",
    "            if(j != (i+node_id)):\n",
    "                edges.append([i+node_id, j])\n",
    "                edge += 1\n",
    "    \n",
    "    # model = building_model()\n",
    "\n",
    "    for (i, segment_val) in enumerate(np.unique(segments_slic)):\n",
    "        \n",
    "\n",
    "        # Mask for the current superpixel\n",
    "        mask = segments_slic == segment_val\n",
    "        \n",
    "        # Find the bounding box of the superpixel\n",
    "        positions = np.where(mask)\n",
    "        top, left = np.min(positions, axis=1)\n",
    "        bottom, right = np.max(positions, axis=1)\n",
    "        \n",
    "        # Extract the superpixel image\n",
    "        superpixel_image = image[top:bottom+1, left:right+1]\n",
    "        mask1 = mask[top:bottom+1, left:right+1]\n",
    "        \n",
    "        # Calculate the size and see if it needs to be increased\n",
    "        current_size = superpixel_image.shape[:2]\n",
    "\n",
    "\n",
    "        resized_superpixel = resize_superpixel(superpixel_image, 32)\n",
    "\n",
    "        PIL_img = Image.fromarray((resized_superpixel * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "        # Apply the transformations to the image\n",
    "        feature_vector = 0\n",
    "        \n",
    "        # feature_vector = extract_features(model, resized_superpixel)\n",
    "        # feature_vector = feature_vector.reshape(1, -1)\n",
    "        # feature_vector = feature_vector.tolist()\n",
    "        # feature_vector = feature_vector[0]\n",
    "\n",
    "        # attrs1.append(feature_vector)\n",
    "\n",
    "        # Assign node label\n",
    "        x, y = centers[i]\n",
    "        mask_check2 = is_coordinate_in_white_region(image_mask, x, y)\n",
    "        if mask_check2 == True:\n",
    "            node_labels.append([node_label, activity_map[node_label]])\n",
    "            patch_file_name = patch_dir + activity_map[node_label] + '/' + str(node_id+1) + '.png'\n",
    "            print( activity_map[node_label])\n",
    "            PIL_img.save(patch_file_name)\n",
    "            node_file_name.append([node_id+1])\n",
    "\n",
    "            image_file_name.append([saved_filename])\n",
    "        else:\n",
    "            patch_file_name = patch_dir +  'normal/' + str(node_id+1) + '.png'\n",
    "            PIL_img.save(patch_file_name)\n",
    "            node_labels.append([3, 'normal'])\n",
    "            node_file_name.append([node_id+1])\n",
    "\n",
    "            image_file_name.append([saved_filename])\n",
    "            \n",
    "\n",
    "        # Assign node id \n",
    "        node_id += 1\n",
    "        cnt     += 1\n",
    "        graph_indicator.append(graph_id)\n",
    "\n",
    "\n",
    "    # attrs1=normalize(attrs1)\n",
    "\n",
    " \n",
    "\n",
    "    print(\"For given image nodes formed: \" + str(cnt)+\" edges formed: \" + str(edge))\n",
    "    # if(cnt != 0): \n",
    "    #     graph_id += 1\n",
    "\n",
    "#generate graphs for all edge-image under given dir along with proper label\n",
    "def generate_graph_with_labels(dirname, label, activity_map):\n",
    "    print(\"\\n... Reading Directory: \" + dirname + \" ...\\n\")\n",
    "    global graph_labels\n",
    "    filenames = glob.glob(dirname + '/*.png')\n",
    "    for filename in filenames:\n",
    "        generate_graphs(filename, label, activity_map)\n",
    "        graph_labels.append([label, activity_map[label]])\n",
    "\n",
    "#generate graphs for all directories\n",
    "def process_graphs(\n",
    "                   benign_dir,\n",
    "                   malignant_dir,\n",
    "                   normal_dir,\n",
    "                   activity_map):\n",
    "    global node_labels, graph_labels\n",
    "    generate_graph_with_labels(benign_dir,  1, activity_map)\n",
    "    generate_graph_with_labels(malignant_dir,  2, activity_map)\n",
    "    generate_graph_with_labels(normal_dir,  3, activity_map)\n",
    "\n",
    "\n",
    "    print(\"Processing done\")\n",
    "    print(\"Total nodes formed: \" + str(len(node_labels)) + \"Total graphs formed: \" + str(len(graph_labels)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#generate_graph_with_labels(BIRAD_0_dir, 1, activity_map)\n",
    "process_graphs( \n",
    "               benign_dir, \n",
    "               malignant_dir, \n",
    "               normal_dir, \n",
    "               activity_map)\n",
    "\n",
    "#check all the lengths of globals\n",
    "#comment if not necessary\n",
    "print(len(node_labels))\n",
    "print(len(graph_labels))\n",
    "print(len(edges))\n",
    "print(len(attrs))\n",
    "print(\"Calculating complete, Start Saving process =================\")\n",
    "\n",
    "\n",
    "#create adjacency dataframe\n",
    "df_A = pd.DataFrame(columns = [\"node-1\", \"node-2\"], data = np.array(edges))\n",
    "print(\"Shape of edge dataframe: \" + str(df_A.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_A.head(50))\n",
    "\n",
    "#create node label dataframe\n",
    "df_node_label = pd.DataFrame(data = np.array(node_labels), columns=[\"label\", \"activity-name\"])\n",
    "print(\"shape of node-label dataframe: \" + str(df_node_label.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_node_label)\n",
    "\n",
    "#create graph label dataframe\n",
    "df_graph_label = pd.DataFrame(data = np.array(graph_labels), columns = [\"label\",\"activity-name\"])\n",
    "print(\"shape of node-label dataframe: \" + str(df_graph_label.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_graph_label.head(50))\n",
    "\n",
    "#create node-attribute dataframe (normalized grayscale value)\n",
    "# df_node_attr = pd.DataFrame(data = np.array(attrs))\n",
    "# print(\"shape of node-attribute dataframe: \" + str(df_node_attr.shape))\n",
    "# print(\"\\n--summary of dataframe--\\n\", df_node_attr.head(50))\n",
    "\n",
    "#create graph-indicator datframe\n",
    "df_graph_indicator = pd.DataFrame(data = np.array(graph_indicator), columns=[\"graph-id\"])\n",
    "print(\"shape of graph-indicator dataframe: \" + str(df_graph_indicator.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_graph_indicator.head(50))\n",
    "\n",
    "#create node file name\n",
    "df_node_file_name = pd.DataFrame(data = np.array(node_file_name))\n",
    "print(\"shape of node-label dataframe: \" + str(df_node_file_name.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", node_file_name)\n",
    "\n",
    "df_image_file_name = pd.DataFrame(data = np.array(image_file_name))\n",
    "print(\"shape of node-label dataframe: \" + str(df_image_file_name.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", image_file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#omit activity name later for graph-label and node-label\n",
    "#since GIN model will only accept the label\n",
    "df_node_label = df_node_label.drop([\"activity-name\"], axis=1)\n",
    "print(df_node_label.head(50))\n",
    "\n",
    "df_graph_label = df_graph_label.drop([\"activity-name\"], axis=1)\n",
    "print(df_graph_label.head(50))\n",
    "\n",
    "\n",
    "\n",
    "def save_dataframe_to_txt(df, filepath):\n",
    "    df.to_csv(filepath, header=None, index=None, sep=',', mode='w')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#save all the dataframes to .txt file\n",
    "#path name: .../GraphTrain/dataset/<dataset_name>/raw/<dataset_name>_<type>.txt\n",
    "# <type>:\n",
    "# A--> adjancency matrix\n",
    "#graph_indicator--> graph-ids of all node\n",
    "#graph_labels--> labels for all graph\n",
    "#node_attributes--> attribute(s) for all node\n",
    "#node_labels--> labels for all node\n",
    "\n",
    "# sourcepath='Dataset/BUSI-with-GT/data-edge/train/raw'\n",
    "# os.makedirs(sourcepath, exist_ok=False)\n",
    "print(\"The new directory is created!\")\n",
    "save_dataframe_to_txt(df_A, sourcepath + '/train_A.txt')\n",
    "save_dataframe_to_txt(df_graph_indicator, sourcepath + '/train_graph_indicator.txt')\n",
    "save_dataframe_to_txt(df_graph_label, sourcepath + '/train_graph_labels.txt')\n",
    "# save_dataframe_to_txt(df_node_attr, sourcepath + '/train_node_attributes.txt')\n",
    "save_dataframe_to_txt(df_node_label, sourcepath + '/train_node_labels.txt')\n",
    "save_dataframe_to_txt(df_node_file_name, sourcepath + '/train_node_file_name.txt')\n",
    "save_dataframe_to_txt(df_image_file_name, sourcepath + '/train_image_file_name.txt')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "time_to_construct = (end - start)/60\n",
    "print(\"Total time (min) for constructing Graph: \", time_to_construct)\n",
    "print(\"=======End constructing Graph process here======\")\n",
    "\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Test Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skimage.data import astronaut\n",
    "from skimage.segmentation import slic\n",
    "from scipy.spatial import Delaunay\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.color import label2rgb\n",
    "from skimage.transform import resize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#define globals required through out the whole program\n",
    "edges           = [] #containing all edge tuple\n",
    "attrs           = [] #countaining list of attribute of all nodes\n",
    "graph_id        = 1 #id of latest graph\n",
    "node_id         = 0 #id of latest node\n",
    "graph_indicator = [] #containing graph-id for each node\n",
    "node_labels     = [] #containing labels for all node\n",
    "graph_labels    = []#containing labels for all graph\n",
    "edge_labels = []\n",
    "edge_attrs = []\n",
    "node_edge_attrs = []\n",
    "\n",
    "# Selecting dataset and k\n",
    "dataset_name = 'BUSI-with-GT'  # 'BUSI-with-GT' or  'BUSBRA' \n",
    "k = 6  # '2' '4' '6' '8'\n",
    "\n",
    "mask_dir = 'Dataset/'+ dataset_name + '/original/Masks/'\n",
    "\n",
    "patch_dir  = 'Dataset/'+ dataset_name + '/data-edge/k' + str(k) + '/test/patch-image/'\n",
    "\n",
    "#working directories\n",
    "\n",
    "benign_dir  = 'Dataset/'+ dataset_name + '/original/test/benign'\n",
    "malignant_dir  = 'Dataset/'+ dataset_name + '/original/test/malignant'\n",
    "normal_dir  = 'Dataset/'+ dataset_name + '/original/test/normal'\n",
    "\n",
    "#Output directories\n",
    "sourcepath='Dataset/'+ dataset_name + '/data-edge/k' + str(k) + '/test/raw'\n",
    "if not os.path.exists(sourcepath):\n",
    "    os.makedirs(sourcepath, exist_ok=False)\n",
    "\n",
    "\n",
    "node_file_name = []\n",
    "image_file_name = []\n",
    "\n",
    "#activity-label vs activity-name mapping (4-class)\n",
    "activity_map    = {}\n",
    "activity_map[1] = 'benign'\n",
    "activity_map[2] = 'malignant'\n",
    "activity_map[3] = 'normal'\n",
    "\n",
    "\n",
    "#\n",
    "def normalize_rows(arr):\n",
    "    min_values = arr.min(axis=1)\n",
    "    max_values = arr.max(axis=1)\n",
    "    normalized_arr = (arr - min_values[:, np.newaxis]) / (max_values - min_values)[:, np.newaxis]\n",
    "    return normalized_arr\n",
    "\n",
    "def find_k_min_indices_desc(array, k):\n",
    "    if k > len(array):\n",
    "        raise ValueError(\"K is larger than the size of the array.\")\n",
    "    sorted_indices = np.argsort(array)\n",
    "    k_min_indices = sorted_indices[:k]\n",
    "    return k_min_indices\n",
    "\n",
    "\n",
    "def resize_superpixel(superpixel, target_size):\n",
    "    # Calculate the resize ratio\n",
    "    \n",
    "\n",
    "    # Resize the superpixel using bilinear interpolation\n",
    "    resized_superpixel = resize(superpixel, (target_size, target_size), \n",
    "                                mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    return resized_superpixel\n",
    "\n",
    "def extract_features(model, img_array):\n",
    "    # img = image.load_img(image_path, target_size=(image_shape[0], image_shape[1]))\n",
    "    # img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    features = model.predict(img_array, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "#z-score normalization\n",
    "def normalize(arr):\n",
    "    arr = np.array(arr)\n",
    "    m   = np.mean(arr)\n",
    "    s   = np.std(arr)\n",
    "    return np.round((arr - m)/s, 3)\n",
    "\n",
    "def is_coordinate_in_white_region(image, x, y):\n",
    "\n",
    "    # Check each coordinate\n",
    "    \n",
    "    # Check if the coordinate is within the image bounds\n",
    "        # Check if the pixel is white (assuming white is represented as 255)\n",
    "    if image[int(x), int(y)] == 255:\n",
    "        \n",
    "        mask_check = True\n",
    "    else: \n",
    "\n",
    "        mask_check = False\n",
    "\n",
    "    return mask_check\n",
    "\n",
    "def replace_prefix(filename, new_prefix):\n",
    "    # Split the filename into prefix and the rest\n",
    "    parts = filename.split('_', 1)\n",
    "    \n",
    "    # If the filename doesn't have a prefix, return it as is\n",
    "    if len(parts) < 2:\n",
    "        return filename\n",
    "    \n",
    "    # Return the new filename with the replaced prefix\n",
    "    return new_prefix + '_' + parts[1]\n",
    "\n",
    "def get_filename(directory):\n",
    "    # Split the directory string by '/'\n",
    "    parts = directory.split('/')\n",
    "    \n",
    "    # The filename is the last part of the directory\n",
    "    filename = parts[-1]\n",
    "    \n",
    "    # Return the filename\n",
    "    return filename\n",
    "\n",
    "#generate graph for a given edge-image file\n",
    "def generate_graphs(filename, node_label, activity_map):\n",
    "    print(\" ... Reading image: \" + filename+\" ...\")\n",
    "    global node_id, edges, attrs, graph_id, node_labels, graph_indicator, edge_labels, edge_attrs, node_edge_attrs, mask_dir, node_file_name, image_file_name, k\n",
    "    cnt           = 0\n",
    "    img           = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    dim1, dim2, _ = img.shape\n",
    "\n",
    "    saved_filename = get_filename(filename)\n",
    "\n",
    "    image_mask_name = get_filename(filename)\n",
    "    image_mask_name = replace_prefix(image_mask_name, 'mask')\n",
    "    image_mask_name = mask_dir + image_mask_name\n",
    "    image_mask = cv2.imread(image_mask_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    attrs1        = []\n",
    "    edge = 0\n",
    "    # Assuming you have an image loaded as 'image'\n",
    "    image = img\n",
    "    # Step 1: Apply SLIC to get initial superpixels\n",
    "    segments_slic = slic(image, n_segments=150, compactness=50)\n",
    "    # Convert segments back to an image\n",
    "    segmented_image = label2rgb(segments_slic, image, kind='avg')\n",
    "\n",
    "    segments_ids = np.unique(segments_slic)\n",
    "\n",
    "    # centers\n",
    "    centers = np.array([np.mean(np.nonzero(segments_slic==i),axis=1) for i in segments_ids])\n",
    "\n",
    "    # Calculate the average intensity for each superpixel\n",
    "    unique_segments = np.unique(segments_slic)\n",
    "    average_intensities = []\n",
    "\n",
    "    print(\"Image type: \" + activity_map[node_label] + \"\\nPixel matrix is of: \" + str(dim1) + \"x\" + str(dim2))\n",
    "\n",
    "    for seg_id in unique_segments:\n",
    "        mask = segments_slic == seg_id\n",
    "        average_intensity = np.mean(image[mask], axis=0)\n",
    "        average_intensity = average_intensity.astype(int)\n",
    "        average_intensities.append(average_intensity)\n",
    "\n",
    "    average_intensities = np.array(average_intensities)\n",
    "\n",
    "    # Compute distances between each pair of superpixels based on Average intensity superpixel\n",
    "    distances_intensity = normalize_rows(cdist(average_intensities, average_intensities, metric='euclidean'))\n",
    "\n",
    "    # Compute distances between each pair of superpixels based on location of Center\n",
    "    distances_pos = normalize_rows(cdist(centers, centers, metric='euclidean'))\n",
    "\n",
    "    # Combine distance\n",
    "    distances = (distances_pos + distances_intensity)/2\n",
    "\n",
    "    for i in unique_segments:       #Scan all the superpixel in image\n",
    "\n",
    "        edges_image = find_k_min_indices_desc(distances[i-1], k+1)\n",
    "        edges_image = edges_image + 1\n",
    "        edges_image = edges_image + node_id\n",
    "\n",
    "        for j in edges_image:        # Forming edges\n",
    "            if(j != (i+node_id)):\n",
    "                edges.append([i+node_id, j])\n",
    "                edge += 1\n",
    "    \n",
    "    # model = building_model()\n",
    "\n",
    "    for (i, segment_val) in enumerate(np.unique(segments_slic)):\n",
    "        \n",
    "\n",
    "        # Mask for the current superpixel\n",
    "        mask = segments_slic == segment_val\n",
    "        \n",
    "        # Find the bounding box of the superpixel\n",
    "        positions = np.where(mask)\n",
    "        top, left = np.min(positions, axis=1)\n",
    "        bottom, right = np.max(positions, axis=1)\n",
    "        \n",
    "        # Extract the superpixel image\n",
    "        superpixel_image = image[top:bottom+1, left:right+1]\n",
    "        mask1 = mask[top:bottom+1, left:right+1]\n",
    "        \n",
    "        # Calculate the size and see if it needs to be increased\n",
    "        current_size = superpixel_image.shape[:2]\n",
    "\n",
    "\n",
    "        resized_superpixel = resize_superpixel(superpixel_image, 32)\n",
    "\n",
    "        PIL_img = Image.fromarray((resized_superpixel * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "        # Apply the transformations to the image\n",
    "        feature_vector = 0\n",
    "        \n",
    "        # feature_vector = extract_features(model, resized_superpixel)\n",
    "        # feature_vector = feature_vector.reshape(1, -1)\n",
    "        # feature_vector = feature_vector.tolist()\n",
    "        # feature_vector = feature_vector[0]\n",
    "\n",
    "        # attrs1.append(feature_vector)\n",
    "\n",
    "        # Assign node label\n",
    "        x, y = centers[i]\n",
    "        mask_check2 = is_coordinate_in_white_region(image_mask, x, y)\n",
    "        if mask_check2 == True:\n",
    "            node_labels.append([node_label, activity_map[node_label]])\n",
    "            patch_file_name = patch_dir + activity_map[node_label] + '/' + str(node_id+1) + '.png'\n",
    "            print( activity_map[node_label])\n",
    "            PIL_img.save(patch_file_name)\n",
    "            node_file_name.append([node_id+1])\n",
    "\n",
    "            image_file_name.append([saved_filename])\n",
    "        else:\n",
    "            patch_file_name = patch_dir +  'normal/' + str(node_id+1) + '.png'\n",
    "            PIL_img.save(patch_file_name)\n",
    "            node_labels.append([3, 'normal'])\n",
    "            node_file_name.append([node_id+1])\n",
    "\n",
    "            image_file_name.append([saved_filename])\n",
    "            \n",
    "\n",
    "        # Assign node id \n",
    "        node_id += 1\n",
    "        cnt     += 1\n",
    "        graph_indicator.append(graph_id)\n",
    "\n",
    "\n",
    "    # attrs1=normalize(attrs1)\n",
    "\n",
    " \n",
    "\n",
    "    print(\"For given image nodes formed: \" + str(cnt)+\" edges formed: \" + str(edge))\n",
    "    # if(cnt != 0): \n",
    "    #     graph_id += 1\n",
    "\n",
    "#generate graphs for all edge-image under given dir along with proper label\n",
    "def generate_graph_with_labels(dirname, label, activity_map):\n",
    "    print(\"\\n... Reading Directory: \" + dirname + \" ...\\n\")\n",
    "    global graph_labels\n",
    "    filenames = glob.glob(dirname + '/*.png')\n",
    "    for filename in filenames:\n",
    "        generate_graphs(filename, label, activity_map)\n",
    "        graph_labels.append([label, activity_map[label]])\n",
    "\n",
    "#generate graphs for all directories\n",
    "def process_graphs(\n",
    "                   benign_dir,\n",
    "                   malignant_dir,\n",
    "                   normal_dir,\n",
    "                   activity_map):\n",
    "    global node_labels, graph_labels\n",
    "    generate_graph_with_labels(benign_dir,  1, activity_map)\n",
    "    generate_graph_with_labels(malignant_dir,  2, activity_map)\n",
    "    generate_graph_with_labels(normal_dir,  3, activity_map)\n",
    "\n",
    "\n",
    "    print(\"Processing done\")\n",
    "    print(\"Total nodes formed: \" + str(len(node_labels)) + \"Total graphs formed: \" + str(len(graph_labels)))\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#generate_graph_with_labels(BIRAD_0_dir, 1, activity_map)\n",
    "process_graphs( \n",
    "               benign_dir, \n",
    "               malignant_dir, \n",
    "               normal_dir, \n",
    "               activity_map)\n",
    "\n",
    "#check all the lengths of globals\n",
    "#comment if not necessary\n",
    "print(len(node_labels))\n",
    "print(len(graph_labels))\n",
    "print(len(edges))\n",
    "print(len(attrs))\n",
    "print(\"Calculating complete, Start Saving process =================\")\n",
    "\n",
    "\n",
    "#create adjacency dataframe\n",
    "df_A = pd.DataFrame(columns = [\"node-1\", \"node-2\"], data = np.array(edges))\n",
    "print(\"Shape of edge dataframe: \" + str(df_A.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_A.head(50))\n",
    "\n",
    "#create node label dataframe\n",
    "df_node_label = pd.DataFrame(data = np.array(node_labels), columns=[\"label\", \"activity-name\"])\n",
    "print(\"shape of node-label dataframe: \" + str(df_node_label.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_node_label)\n",
    "\n",
    "#create graph label dataframe\n",
    "df_graph_label = pd.DataFrame(data = np.array(graph_labels), columns = [\"label\",\"activity-name\"])\n",
    "print(\"shape of node-label dataframe: \" + str(df_graph_label.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_graph_label.head(50))\n",
    "\n",
    "#create node-attribute dataframe (normalized grayscale value)\n",
    "# df_node_attr = pd.DataFrame(data = np.array(attrs))\n",
    "# print(\"shape of node-attribute dataframe: \" + str(df_node_attr.shape))\n",
    "# print(\"\\n--summary of dataframe--\\n\", df_node_attr.head(50))\n",
    "\n",
    "#create graph-indicator datframe\n",
    "df_graph_indicator = pd.DataFrame(data = np.array(graph_indicator), columns=[\"graph-id\"])\n",
    "print(\"shape of graph-indicator dataframe: \" + str(df_graph_indicator.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", df_graph_indicator.head(50))\n",
    "\n",
    "#create node file name\n",
    "df_node_file_name = pd.DataFrame(data = np.array(node_file_name))\n",
    "print(\"shape of node-label dataframe: \" + str(df_node_file_name.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", node_file_name)\n",
    "\n",
    "df_image_file_name = pd.DataFrame(data = np.array(image_file_name))\n",
    "print(\"shape of node-label dataframe: \" + str(df_image_file_name.shape))\n",
    "print(\"\\n--summary of dataframe--\\n\", image_file_name)\n",
    "\n",
    "\n",
    "#omit activity name later for graph-label and node-label\n",
    "#since GIN model will only accept the label\n",
    "df_node_label = df_node_label.drop([\"activity-name\"], axis=1)\n",
    "print(df_node_label.head(50))\n",
    "\n",
    "df_graph_label = df_graph_label.drop([\"activity-name\"], axis=1)\n",
    "print(df_graph_label.head(50))\n",
    "\n",
    "\n",
    "\n",
    "def save_dataframe_to_txt(df, filepath):\n",
    "    df.to_csv(filepath, header=None, index=None, sep=',', mode='w')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#save all the dataframes to .txt file\n",
    "#path name: .../GraphTrain/dataset/<dataset_name>/raw/<dataset_name>_<type>.txt\n",
    "# <type>:\n",
    "# A--> adjancency matrix\n",
    "#graph_indicator--> graph-ids of all node\n",
    "#graph_labels--> labels for all graph\n",
    "#node_attributes--> attribute(s) for all node\n",
    "#node_labels--> labels for all node\n",
    "\n",
    "# sourcepath='Dataset/BUSI-with-GT/data-edge/test/raw'\n",
    "# os.makedirs(sourcepath, exist_ok=False)\n",
    "print(\"The new directory is created!\")\n",
    "save_dataframe_to_txt(df_A, sourcepath + '/test_A.txt')\n",
    "save_dataframe_to_txt(df_graph_indicator, sourcepath + '/test_graph_indicator.txt')\n",
    "save_dataframe_to_txt(df_graph_label, sourcepath + '/test_graph_labels.txt')\n",
    "# save_dataframe_to_txt(df_node_attr, sourcepath + '/train_node_attributes.txt')\n",
    "save_dataframe_to_txt(df_node_label, sourcepath + '/test_node_labels.txt')\n",
    "save_dataframe_to_txt(df_node_file_name, sourcepath + '/test_node_file_name.txt')\n",
    "save_dataframe_to_txt(df_image_file_name, sourcepath + '/test_image_file_name.txt')\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "time_to_construct = (end - start)/60\n",
    "print(\"Total time (min) for constructing Graph: \", time_to_construct)\n",
    "print(\"=======End constructing Graph process here======\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
